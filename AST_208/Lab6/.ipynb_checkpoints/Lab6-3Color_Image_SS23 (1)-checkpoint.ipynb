{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AST 208 Lab 6: Three-Color Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i class=\"fa fa-pencil\" style=\"font-size:1.5em; color:red\"></i>\n",
    "Your Name  \n",
    "Date  \n",
    "**Team Name:**   \n",
    "**Collaborators:** \n",
    "\n",
    "50 Total Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction\n",
    "In this lab, we'll investigate images of an astronomical object and do through the process of calibrating and combining images of it in multiple filters. \n",
    "\n",
    "We will reduce our \"pretty\" images of Messier 1. This is also known as the \"Crab Nebula\" A supernova remnant. By \"reduce\", we mean that we shall clean the data of instrumental artifacts, by subtract off the bias and dark current, and then use the twilight flat to remove imperfections or distortions from our image.  We will them make  a 3-color image with DS9. \n",
    "\n",
    "We need to load in our usual plotting and numpy packages. In addition, we need to load a module to read in the FITS image files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i class=\"fa fa-cogs\" style=\"font-size:1.5em; color:red\"></i> 1) What is the exposure time for each science image?\n",
    "\n",
    "<span style=\"color:red\">(2 points)</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in your images\n",
    "To manipulate an image file in Python, first we need to load the image file into a numpy array. The basic proceedure to do this is\n",
    "\n",
    "1. open the file—meaning, locate the file on the disk and prepare memory for loading it;\n",
    "2. extract the data and store it in an array;\n",
    "3. close the file—meaning, release any system resources that were used to read the file from disk.\n",
    "\n",
    "The following lines demonstrate this\n",
    "```python\n",
    "# open the fits file, and give it a \"handle\", frame.\n",
    "frame = fits.open('bias1.fits')\n",
    "# extract the data from frame, and store it in a numpy array called 'bias'\n",
    "bias = np.array(frame[0].data,dtype=np.float64)\n",
    "# close frame\n",
    "frame.close()\n",
    "```\n",
    "The first line opens the file 'bias1.fits' for reading and gives the opened file an alias, `frame`. The fits file is general and can store more than one image; this is why we write `frame[0]` (although in this case there is only one image).  Each image has a header, containing information about the image, as well as the raw data.  To access the data for the first image in `frame`, we therefore write `frame[0].data`. We want to convert the numbers to 64-bit floating point, so we add the option `dtype=np.float64`.\n",
    "\n",
    "As you learned last week, each frame should be $1024\\times1024$ pixels: to verify this, we can find the *shape* of the array like so\n",
    "```python\n",
    "print('shape of bias is ',bias.shape)\n",
    "```\n",
    "The shape of this array should be (1024, 1024). Here, `bias[42,35]` refers to the pixel in row 42, column 35 (remember that arrays are indexed starting with 0).  \n",
    "\n",
    "<i class=\"fa fa-cogs\" style=\"font-size:1.5em; color:red\"></i> 2) In the following cell, load the eight fits files into eight logically named python arrays. Add a print statment for each array to tell us the shape of each frame.\n",
    "\n",
    "<span style=\"color:red\">(4 points)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct for CCD Bias level\n",
    "<i class=\"fa fa-cogs\" style=\"font-size:1.5em; color:red\"></i> 3) In last week's lab, you estimated the typical pixel value of the bias frame with statistics in DS9 (Lab 5, #6). In the code cell below, use array math in python to calculate the equivalent statistic for the bias frame (Recall `np.mean` and `np.median` and print both. This time, you can do the statistics on the whole bias frame, not just a region). Print out these statistics with a print statement (don't forget your units!). How does the number you find here compare with what you found last week?\n",
    "\n",
    "<span style=\"color:red\">(3 points)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i class=\"fa fa-cogs\" style=\"font-size:1.5em; color:red\"></i> 4) In the code cell below, subtract the bias image from each of the three science images ($B$, $V$, and $R$). Don't just subtract the average bias level, but subtract the bias *image* (i.e., so that you are subtracting the value of bias pixel[1,1] from the value of science pixel [1,1], and the  value of bias pixel[72,89] from the value of science pixel [72,89], and so on for every pixel).  \n",
    "\n",
    "<span style=\"color:red\">(2 points)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i class=\"fa fa-cogs\" style=\"font-size:1.5em; color:red\"></i> 5) Print the median values of the three science images before and after bias subtraction. Is this consistent with what you would expect from #3? Don't forget to explain if your python calculations are consistent with expectations and specify units!\n",
    "\n",
    "<span style=\"color:red\">(3 points)</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct for Dark Current\n",
    "\n",
    "Recall from Lab 5 that he SkyNet dark frame already has had the bias subtracted from it (this step is recorded in the header, denoted as `BIASCORR`)---so the counts in your dark image only represent dark counts. \n",
    "\n",
    "<i class=\"fa fa-pencil\" style=\"font-size:1.5em; color:red\"></i> 6) Recall that dark current accumulates in the CCD with time; the longer you expose, the more counts there will be due to dark current. Last week, you found the exposure time of your dark image (Lab 5, #9). In #1 of this lab, you found the exposure time(s) for your science images.  Normally darks are taken at the same exposure time as the science image but in this case we forgot...oops. What factors do you need to scale the dark image by to apply it to your $B$, $V$, and $R$ science frames? \n",
    "\n",
    "<span style=\"color:red\">(3 points)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i class=\"fa fa-cogs\" style=\"font-size:1.5em; color:red\"></i> 7) In the code cell below, subtract the scaled dark image from the **bias-subtracted** $B$, $V$, and $R$ science frames. \n",
    "\n",
    "<span style=\"color:red\">(3 points total)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<i class=\"fa fa-cogs\" style=\"font-size:1.5em; color:red\"></i> 8) Print the median values of the three science images after dark subtraction. How does this compare with the bias-subtracted median values from #5? Does this make sense, based on what you estimated in #6 and the dark statistics you measured in Lab 5?\n",
    "\n",
    "<span style=\"color:red\">(4 points)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct for Flat Field\n",
    "\n",
    "Recall from Lab 5 that the flat field image you download from SkyNet has already had the bias frame and dark frame subtracted from it (as seen by `BIASCORR`, `DARKCORR`, and `DARKSCAL` in the header).\n",
    "\n",
    "To correct our science image, we don't want to subtract the flat image; rather, we want to divide by the flat image. To do this without reducing the brightness of our science image, we'll first *normalize* our flat image so that its median pixel value is 1. That is, if everything were uniform and our flat image was truly flat, then the science image would not change when we divide by the flat.\n",
    "\n",
    "<i class=\"fa fa-cogs\" style=\"font-size:1.5em; color:red\"></i> 9) For each of the three flat frames ($B$, $V$, and $R$), normalize it by dividing it by its median pixel value. Confirm that the median value of each normalized flat is 1. Do not manually type the median value into your code, use python functions.\n",
    "\n",
    "<span style=\"color:red\">(4 points)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i class=\"fa fa-cogs\" style=\"font-size:1.5em; color:red\"></i> 10) Divide each of the three bias- and dark-subtracted science frames ($B$, $V$, and $R$) by the appropriate normalized flat. \n",
    "\n",
    "<span style=\"color:red\">(2 points)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i class=\"fa fa-cogs\" style=\"font-size:1.5em; color:red\"></i> 11) Confirm that the median value of each science image has not been altered much by flat-fielding, by comparing median values with your answer from #8. \n",
    "\n",
    "<span style=\"color:red\">(2 points)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i class=\"fa fa-cogs\" style=\"font-size:1.5em; color:red\"></i> 12) Export each of the flat-fielded science image arrays (`corrected image` in below command) to a fits file (which you can name whatever---in the below code it is called `newfile.fits`) with:\n",
    "```python\n",
    "fits.writeto('newfile.fits', corrected_image, overwrite=True)\n",
    "```\n",
    "\n",
    "<span style=\"color:red\">(3 points)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i class=\"fa fa-pencil\" style=\"font-size:1.5em; color:red\"></i> 13) View these corrected (or \"reduced\") fits images into DS9, and compare with the original \"raw\" science images.  How have your images changed, based on the corrections above? Points to consider:\n",
    "\n",
    "A. How has the typical value of the background changed, in the reduced image compared to the original image? \n",
    "\n",
    "B. In the final reduced image, what is the source of the remaining background counts? \n",
    "\n",
    "C. Are the \"checkerboard\" pattern and/or dust donuts gone in the reduced image? Which step(s) of the data reduction process affects these structures?\n",
    "\n",
    "\n",
    "<span style=\"color:red\">(4 points)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i class=\"fa fa-image\" style=\"font-size:1.5em; color:red\"></i> 14) Find an example of an artifact in the raw image that is corrected by flat fielding. Embed the \"before\" and \"after\" correction images here. Label the before and after images (Mac Preview can do this). \n",
    "\n",
    "<span style=\"color:red\">(3 points)</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make your beautiful image!\n",
    "\n",
    "When you exported your images from python, we did not export the header, and therefore lost the  world coordinate system (WCS). While it's totally possible to export the header as part of the fits file (see the [astropy documentation](https://docs.astropy.org/en/stable/io/fits/api/files.html#astropy.io.fits.writeto)), we can probably get a higher quality WCS by using astrometry.net. Upload your newly-exported fits files to http://nova.astrometry.net/upload.\n",
    " When you reload it into DS9, after applying a WCS, you should see 'WCS' or 'fk5' coordinates that pop up giving RA and Dec.\n",
    "\n",
    "Load each of your three images ($B$, $V$, and $R$) into a separate frame in DS9 ('Frame > New Frame'), and then go to 'Frame > Lock > Frame > WCS'. Blink between the frames to make sure they are lined up. Zoom in on a star and blink again to double-check that they are lined up.\n",
    "\n",
    "If the images are not lined up, then there is an issue with the WCS. This will not be the case here but if it were, you could try uploading your image to http://nova.astrometry.net/upload to see if the algorithm there can fix your WCS. If that didn't work, below is how you could do it in python. This is not needed for the images provided.\n",
    "\n",
    "<i class=\"fa fa-exclamation-triangle\" style=\"font-size:1.5em; color:red\"></i> **If you need to shift an image in python:**\n",
    "\n",
    "1. import the `shift` function:\n",
    "```python\n",
    "from scipy.ndimage import shift\n",
    "```\n",
    "\n",
    "2. use DS9 to figure out how much of a shift in the $x$ and $y$ directions is needed;\n",
    "\n",
    "3. load the image into this notebook:\n",
    "```python\n",
    "frame = fits.open('file.fits')\n",
    "im = np.array(frame[0].data,dtype=np.float64)\n",
    "frame.close()\n",
    "```\n",
    "\n",
    "4. shift the image: for example\n",
    "```python\n",
    "shifted_image = shift(im,[20,25])\n",
    "```\n",
    "produces an image shifted by 20 pixels to the right and 25 pixels upward.\n",
    "\n",
    "5. save the new image to a fits file\n",
    "```python\n",
    "fits.writeto('newfile.fits', shifted_image, overwrite=True)\n",
    "```\n",
    "\n",
    "In these instructions `file` and `newfile` mean whatever the file name is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Once you are happy that your three images are aligned, in DS9, select `New Frame RGB`. A window should pop up with rows of 'Red', 'Green', and 'Blue'. DO NOT CLOSE IT! You'll be loading in three different images, one for red, one for green, and one for blue. Whichever color is selected under the 'Current' column is the color that you can mess around with---if you load an image, you'll be loading it in as that color, and if you mess with the colorbar, you are only messing with the colorbar of that color.\n",
    "\n",
    "Load in your three science images by successively changing the color selected under 'Current', and just opening the images with 'File > Open', as you usually do. Make sure to load the *R* filter image in as red color, *V* as green, and *B* as blue.\n",
    "\n",
    "To make your 3-color image pretty, mess with the image scales and color bars of each of the three images independently or together, until you get an image in which red, green, and blue are all represented. The goal is to have different parts of your image look like different colors.\n",
    "\n",
    "<i class=\"fa fa-image\" style=\"font-size:1.5em; color:red\"></i> 15)  Export your image and embed it below. Which parts of your image appear bluest? \n",
    "\n",
    "<span style=\"color:red\">(8 points)</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <i class=\"fa fa-exclamation-triangle\" style=\"font-size:1.5em; color:red\"></i>Closeout\n",
    "\n",
    "You will need to include image files when you make a tarball of this lab. In addition, you will need to include any jpg/png images you embed in this lab. Remove any \"tips\" and unnecessary instruction text or cells. When you are finished, this notebook should only include numbered questions and your answers. Make sure all cells are rendered, and that this notebook is saved.  After you've saved it, select `File : Close and Halt`. Place this notebook file and **any** image files that you used into this notebook into a separate folder.  Follow the instructions on D2L for archiving and uploading your lab report to the dropbox."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
